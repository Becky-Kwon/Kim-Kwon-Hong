{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  🍔 요기요 데이터 수집\n",
    "\n",
    "## 사전 자체 데이터를 수집하기 위해 요기요 웹페이지 대상으로 크롤링을 수행한다. \n",
    "### 수집하는 과정은 다음과 같다.\n",
    "- 리뷰 데이터 수집을 위한 사전단계로서 가게별 URL 수집\n",
    "- 가게별 URL 기반 리뷰 데이터 수집\n",
    "- 추가 분석을 위한 가게 평점, 메뉴 등 전반적인 가게 정보 수집\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 요기요 가게별 URL 수집\n",
    "\n",
    "## 개요\n",
    "요기요 홈페이지는 대부분의 요소가 동적 페이지로 구성되어 있다. 이에 따라 발생하는 이슈를 해결하며, 데이터 수집의 효율성을 위해 가게별 접근이 직접적으로 가능하도록 리뷰 데이터에 수집에 앞서 가게별로 URL을 수집한다. \n",
    "\n",
    "\n",
    "- 먼저 URL을 수집한 첫번째 이유는 하나의 가게 리뷰를 수집한 후 기존의 가게 리스트로 돌아가게 될 경우 동적으로 요소들이 로딩되어 기존의 가게 리스트 순서가 보장되지 않는 문제를 해결하기 위함이다. 가게 리스트의 순서가 보장되지 않는 이유는 시간대에 따라 현재 오픈 중인 가게가 동적으로 리스트 상위에 위치하게 되기 때문이며, 수집하는 양과 소요되는 시간을 고려하였을 때 중복 또는 누락되는 가게가 생길 가능성이 크다. \n",
    " \n",
    " \n",
    "\n",
    "- 두번째 이유는 요기요의 가게 리스트가 페이지 마지막 스크롤마다 60개씩 새로 생성됨에 따라 발생하는 비효율성을 해결하기 위함이다. 요기요 페이지는 하나의 카테고리당 많게는 수백, 수천개의 가게가 존재한다. 이에 하단에 위치하는 가게에 접근하기 위해서는 상당히 많은 횟수의 스크롤이 필요하며, 이 과정에서 네트워크 지연, 인터넷 사용기록의 과다한 적재 등의 문제가 발생하였을 때는 예외처리로도 복구하기 어려운 작업 손실과 시간 소요가 발생한다. 특히 첫번째 이유에서 언급했듯 가게 리스트가 동적으로 요소들이 재로딩됨에 따라 하나의 가게 수집 후 다시 가게 리스트로 돌아오는 경우 다시 첫번째 가게부터 다음 수집 대상 가게까지 스크롤을 해야하는 상황이 벌어진다. 이는 후반부의 가게로 갈수록 시간이 급격하게 늘어나 큰 비효율을 야기하게 된다.\n",
    " \n",
    " \n",
    "- 이에 초기에 URL을 미리 수집해둔 후 가게별로 접근할 때 사용함으로써 큰 시간 절감 효과를 얻을 수 있다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카테고리별 가게 개수 수집\n",
    "\n",
    "수집을 원하는 지역과 카테고리를 입력 받아 해당 지역으로 지역설정을 변경한다. 이후 해당 카테고리에 접속하여 리뷰 많은 순으로 정렬 후 스크롤을 끝까지 내려 해당 카테고리 내에 가게가 몇개 존재하는지 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_reviews(location, food_category):\n",
    "    location\n",
    "    name = location.split(\" \")[1]\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "    urls = []\n",
    "    df = pd.DataFrame(columns=['url', '개수충족'])\n",
    "\n",
    "    #### 주소 기준으로 초기화\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "\n",
    "    driver.get(list_url)\n",
    "    time.sleep(8)\n",
    "    element = driver.find_element_by_name(\"address_input\")\n",
    "    element.clear()\n",
    "    element.send_keys(location)\n",
    "    btn = driver.find_element_by_css_selector(\"#button_search_address > button.btn.btn-default.ico-pick\")\n",
    "    btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 리뷰 많은 순으로 sorting\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select/option[3]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # 카테고리 접속 및 가게 리스트 \n",
    "    driver.get(list_url + food_category)\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    print(last_height)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            element.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        time.sleep(1)\n",
    "        if new_height == last_height:\n",
    "            print(\"끝\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "    \n",
    "    store_list = driver.find_elements_by_xpath(\"//div[@class='item clearfix']\")\n",
    "    \n",
    "    return len(store_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가게별 URL 수집\n",
    "\n",
    "수집을 원하는 지역, 카테고리, 시작번호, 끝번호를 인자로 받아 URL 정보를 수집한다. 이 때 href 태그를 통한 URL 수집이 불가능하므로 하나의 가게에 직접 접속 후 다시 되돌아가는 작업이 필요하게 되고, 앞서 개요에서 언급한 두번째 단점을 수반하는 것이 불가피하다. 이로 인해 발생하는 수많은 스크롤 조작에 의한 부하를 방지하기 위하여 100개씩 URL을 수집하며, 이를 위하여 앞서 수집한 카테고리별 가게 개수 데이터를 활용한다. \n",
    "\n",
    "이 때 수집되는 데이터는 \n",
    "- 가게별 URL\n",
    "- 가게명\n",
    "- 리뷰개수 충족 여부이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url3(location, food_category, start, end):\n",
    "    name = location.split(\" \")[1]\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "    urls = []\n",
    "    parsed_data = pd.read_csv(\"./강남구url/강남구_\"+food_category+\"_url.csv\")\n",
    "    stores = parsed_data['가게명'].tolist()\n",
    "    df = pd.DataFrame(columns=['url', '개수충족','가게명'])\n",
    "    \n",
    "    #### 주소 기준으로 초기화\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "\n",
    "    driver.get(list_url)\n",
    "    time.sleep(8)\n",
    "    element = driver.find_element_by_name(\"address_input\")\n",
    "    element.clear()\n",
    "    element.send_keys(location)\n",
    "    btn = driver.find_element_by_css_selector(\"#button_search_address > button.btn.btn-default.ico-pick\")\n",
    "    btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 리뷰 많은 순으로 sorting\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select/option[3]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # 카테고리 접속 및 가게 리스트 \n",
    "    driver.get(list_url + food_category) \n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        for i in tqdm(range(start, end)):\n",
    "            time.sleep(0.5)\n",
    "            is_valid = 1\n",
    "            path = '//*[@id=\"content\"]/div/div[5]/div/div/div[' + str(i) + ']/div'\n",
    "            \n",
    "            # 가게명 요소 찾기(스크롤이 필요 없는 초기 60개 리스트)\n",
    "            try:\n",
    "                store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[5]/div/div/div['+str(i)+']/div/table/tbody/tr/td[2]/div/div[1]').text\n",
    "            \n",
    "            # 지정 횟수만큼 스크롤    \n",
    "            except:\n",
    "                for j in range(int(i // 60)):\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    if i<480:\n",
    "                        time.sleep(i//60)\n",
    "                    else:\n",
    "                        time.sleep(8)\n",
    "            \n",
    "            # 가게명과 리뷰 개수 찾기           \n",
    "            try:\n",
    "                store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[5]/div/div/div['+str(i)+']/div/table/tbody/tr/td[2]/div/div[1]').text\n",
    "                n = driver.find_element_by_css_selector(\n",
    "                    \"#content > div > div:nth-child(5) > div > div > div:nth-child(\" + str(\n",
    "                        i) + \") > div > table > tbody > tr > td:nth-child(2) > div > div.stars > span:nth-child(2)\").text\n",
    "            \n",
    "            # 찾지 못한다면 네트워크 부하로 아직 정보가 나타나지 않았으므로 대기 및 재스크롤 후 요소 찾음 \n",
    "            except:\n",
    "                time.sleep(6)\n",
    "                store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[5]/div/div/div['+str(i)+']/div/table/tbody/tr/td[2]/div/div[1]').text\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                n = driver.find_element_by_css_selector(\n",
    "                    \"#content > div > div:nth-child(5) > div > div > div:nth-child(\" + str(\n",
    "                        i) + \") > div > table > tbody > tr > td:nth-child(2) > div > div.stars > span:nth-child(2)\").text\n",
    "            \n",
    "            # 이미 중복된 가게라면 넘어가기\n",
    "            print(store_name)\n",
    "            if store_name in stores:\n",
    "                print(\"skip\")\n",
    "                continue\n",
    "             \n",
    "            # 리뷰 개수가 10개 이상인지 확인\n",
    "            some_tag = driver.find_element_by_xpath(path)\n",
    "            if n == \"\" or int(n.split(\" \")[1]) < 10:\n",
    "                is_valid = 0\n",
    "\n",
    "            # somthing element 까지 스크롤\n",
    "            # 해당 페이지 접속\n",
    "            action = ActionChains(driver)\n",
    "            action.move_to_element(some_tag).perform()\n",
    "            driver.find_element_by_xpath(path).click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            row = pd.DataFrame([(driver.current_url, is_valid, store_name)],columns=['url', '개수충족','가게명'])\n",
    "            print(row)\n",
    "            df = df.append(row)\n",
    "\n",
    "            driver.back()\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # 예외처리로 해결할 수 없는 문제 발생 시 현재까지의 정보 저장 \n",
    "    except:\n",
    "        df.to_csv(\"./temp/\"+location.split(\" \")[1]+\"_\"+food_category+\"_url_list2_비정상종료-\"+str(start)+\".csv\",\n",
    "              encoding='utf-8-sig')\n",
    "        \n",
    "    # 결과 저장    \n",
    "    df.to_csv(\"./temp/\"+location.split(\" \")[1]+\"_\"+food_category+\"_url_list2-\"+str(start)+\".csv\",\n",
    "              encoding='utf-8-sig')\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가게 이름 수집\n",
    "\n",
    "초기 URL 수집 메소드는 이름을 저장하지 않아 수집된 URL에 접속해 가게명을 추출하는 과정을 거쳤다. 위의 get_url3로 변경된 후 사용하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(category):\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "    df = pd.read_csv(\"./강남구_\"+category+\"_url_list.csv\")\n",
    "    urls = df['url'].tolist()\n",
    "    store_name = []\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            store_name.append(driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[1]/span').text)\n",
    "        except:\n",
    "            print(url)\n",
    "    df['가게명']=store_name\n",
    "    df.to_csv(\"./강남구_\"+category+\"_url_list.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 가게 개수 수집 \n",
    "dic = {}\n",
    "categories = [\"프랜차이즈\", \"한식\", \"치킨\", \"피자양식\", \"중식\", \"일식돈까스\", \"족발보쌈\", \"야식\", \"분식\", \"카페디저트\", \"편의점\"]\n",
    "\n",
    "for cat in categories:\n",
    "    dic[cat] = num_of_reviews(\"서울특별시 강남구 삼성동 16-1 강남구청\", cat)\n",
    "    \n",
    "'''\n",
    "결과 \n",
    "\n",
    "dic = {'1인분주문': 530,\n",
    " '프랜차이즈': 818,\n",
    " '치킨': 331,\n",
    " '피자양식': 571,\n",
    " '중식': 210,\n",
    " '한식': 1614,\n",
    " '일식돈까스': 607,\n",
    " '족발보쌈': 126,\n",
    " '야식': 677,\n",
    " '분식': 666,\n",
    " '카페디저트': 727,\n",
    " '편의점': 80}\n",
    "\n",
    "'''\n",
    "\n",
    "# 카테고리별로 url 수집\n",
    "# 맨 아래까지 스크롤 시 60개씩 동적으로 가게 추가 표시되어 컴퓨터에 상당한 무리가 가며, 해당 가게에 접속하여 url 수집 후 되돌아오면 세션이 초기화 됨\n",
    "# 에러 핸들링을 위하여 100개씩 나누어서 수집\n",
    "for cat in categories:\n",
    "    review_num = dic[cat]\n",
    "    temp = review_num\n",
    "    n = 1\n",
    "    for i in range(review_num//100+1):\n",
    "        print(n, temp)\n",
    "        if temp < 100:\n",
    "            get_url3(\"서울특별시 강남구 삼성동 16-1 강남구청\", cat, n, review_num+1)\n",
    "            break\n",
    "        get_url3(\"서울특별시 강남구 삼성동 16-1 강남구청\", cat, n, n+100)\n",
    "        n+=100\n",
    "        temp = review_num - n\n",
    "        \n",
    "\n",
    "# 수집된 url 데이터셋에 가게이름 추가\n",
    "for category in categories:\n",
    "    try:\n",
    "        get_name(category)\n",
    "    except:\n",
    "        print(category)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 요기요 리뷰 크롤링\n",
    "## 개요\n",
    "\n",
    "앞서 수집한 URL 데이터를 기반으로 가게별 리뷰 데이터를 수집한다. 11개의 업종 카테고리에 대해서 기준에 충족하는 가게 페이지로 접속 후, 한 건의 리뷰에 대하여 다음과 같은 데이터를 수집한다.\n",
    "- 시 \n",
    "- 구\n",
    "- 업종명\n",
    "- 가게명\n",
    "- 년\n",
    "- 월\n",
    "- 전체 평점\n",
    "- 맛 평점\n",
    "- 양 평점\n",
    "- 배달 평점\n",
    "- 리뷰 내역\n",
    "- 주문 내역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 리뷰 데이터 크롤링\n",
    "지역과 카테고리를 인자로 받아 해당 지역/업종에 대해 URL 기반 리뷰를 수집한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yogiyo_crawling(location, food_category):\n",
    "    \n",
    "    # url 가져오기\n",
    "    urls = pd.read_csv(\"./\"+location.split(\" \")[1]+\"_\"+food_category+\"_url_list.csv\")\n",
    "    \n",
    "    # 유효한 가게(리뷰개수 10개 이상)만 가져오기 \n",
    "    is_valid = urls['개수충족'] == 1\n",
    "    urls = urls[is_valid]\n",
    "    urls = urls['url'].values.tolist()\n",
    "    \n",
    "    address = location\n",
    "    name = location.split(\" \")[1]\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "    \n",
    "    # 결과 데이터셋 생성\n",
    "    df = pd.DataFrame(columns=['시', '구', '업종명', '가게명', '년', '월', '전체평점', '맛 평점', '양 평점', '배달 평점', '리뷰 내용', '주문 내역'])\n",
    "\n",
    "    try:\n",
    "        # 가게별로 접속하여 리뷰 수집\n",
    "        for url in tqdm(urls):\n",
    "            # 해당 가게 url로 접속\n",
    "            driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # 리뷰 개수\n",
    "            num_of_review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[2]/div/strong[1]')\n",
    "            \n",
    "            # 가게명\n",
    "            store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[1]/span').text\n",
    "            \n",
    "            # 리뷰를 볼 수 있는 버튼 찾고 클릭  \n",
    "            try:\n",
    "                review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a')\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a')\n",
    "\n",
    "            driver.execute_script(\"arguments[0].click();\", review)\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 전체 리뷰 개수 확인    \n",
    "            try:\n",
    "                num_of_review = driver.find_element_by_xpath(\n",
    "                    '//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[2]/div/strong[1]')\n",
    "                if num_of_review == '':\n",
    "                    num_of_review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a/span')\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                num_of_review = driver.find_element_by_xpath(\n",
    "                    '//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[2]/div/strong[1]')\n",
    "                if num_of_review == '':\n",
    "                    num_of_review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a/span')\n",
    "\n",
    "                    \n",
    "            # 동적으로 변하는 더보기 버튼의 인덱스 계산(더보기 클릭 시 10개씩 증가) 및 전체 리뷰 확인 가능할 때 까지 클릭      \n",
    "            j = 0\n",
    "            idx = 11\n",
    "            max_idx = int(num_of_review.text) // 10\n",
    "\n",
    "            while True:\n",
    "                if j == max_idx:\n",
    "                    html = driver.page_source\n",
    "                    break\n",
    "                try:\n",
    "                    driver.find_element_by_css_selector('#review > li.list-group-item.btn-more > a').click()\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "                    dates = driver.find_element_by_xpath(\n",
    "                        \"// *[ @ id = 'review'] / li[\" + str(idx) + \"] / div[1] / span[2]\").text\n",
    "                    if \"전\" in dates or \"어제\" in dates or int(dates.split(\"년\")[0]) > 2019:\n",
    "                        pass\n",
    "                    elif int(dates.split(\"년\")[0]) == 2019 and int(dates.split(\" \")[1][:-1]) >= 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        html = driver.page_source\n",
    "                        break\n",
    "\n",
    "                except Exception as e: # 네트워크 지연에 따른 예외 발생 시 대기 후 재실행\n",
    "                    print(e)\n",
    "                    time.sleep(3)\n",
    "                    driver.find_element_by_css_selector('#review > li.list-group-item.btn-more > a').click()\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "                    dates = driver.find_element_by_xpath(\n",
    "                        \"// *[ @ id = 'review'] / li[\" + str(idx) + \"] / div[1] / span[2]\").text\n",
    "                    if \"전\" in dates or \"어제\" in dates or int(dates.split(\"년\")[0]) > 2019:\n",
    "                        pass\n",
    "                    elif int(dates.split(\"년\")[0]) == 2019 and int(dates.split(\" \")[1][:-1]) >= 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        html = driver.page_source\n",
    "                        break\n",
    "                j += 1\n",
    "                idx += 10\n",
    "\n",
    "            # 전체 리뷰 확보 후 html 파싱    \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # 리뷰만 가져오기 \n",
    "            reviews = soup.find_all(class_='list-group-item star-point ng-scope')\n",
    "            \n",
    "            # 데이터 삽입 전 전처리\n",
    "            for review in reviews:\n",
    "                # 날짜 계산\n",
    "                if \"전\" in review.find(class_=\"review-time ng-binding\").get_text() or (\n",
    "                        \"어제\" in review.find(class_=\"review-time ng-binding\").get_text()):\n",
    "                    year = 2021\n",
    "                    month = datetime.date.today().month\n",
    "                else:\n",
    "                    year = int(review.find(class_=\"review-time ng-binding\").get_text().split(\"년\")[0])\n",
    "                    month = int(review.find(class_=\"review-time ng-binding\").get_text().split(\" \")[1][0])\n",
    "\n",
    "                # 리뷰     \n",
    "                comment = review.find('p', attrs={'ng-show': 'review.comment'})\n",
    "                comment = comment.get_text().replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "                \n",
    "                # 주문 메뉴 \n",
    "                menu = review.find('div', attrs={'class': \"order-items default ng-binding\"}).get_text()\n",
    "                \n",
    "                # 전체 평점 \n",
    "                overall = review.find_all(class_=\"full ng-scope\")\n",
    "                \n",
    "                # 세부 평점 \n",
    "                points = review.find_all(class_=\"points ng-binding\")\n",
    "                try:\n",
    "                    taste = int(points[0].get_text()) # 맛평점 \n",
    "                except:\n",
    "                    taste = \"\"\n",
    "                try:\n",
    "                    quantity = int(points[1].get_text()) # 양평점 \n",
    "                except:\n",
    "                    quantity = \"\"\n",
    "                try:\n",
    "                    delivery = int(points[2].get_text()) # 배달 평점\n",
    "                except:\n",
    "                    delivery = \"\"\n",
    "\n",
    "                # 하나의 리뷰 데이터 구성     \n",
    "                row = pd.DataFrame(\n",
    "                    [(address.split(\" \")[0], address.split(\" \")[1], food_category, store_name, year, month,\n",
    "                      len(overall), taste, quantity, delivery, comment, menu)],\n",
    "                    columns=['시', '구', '업종명', '가게명', '년', '월', '전체평점', '맛 평점', '양 평점', '배달 평점', '리뷰 내용', '주문 내역'])\n",
    "                \n",
    "                # 최종 데이터셋에 추가\n",
    "                df = df.append(row)\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "    # 지나치게 많은 리뷰와 네트워크 지연 등의 문제로 동적 페이지 크롤링의 오류 다수 증가, 종료된 위치와 중간 결과를 저장함으로써 대처        \n",
    "    except Exception as e:\n",
    "        name = location.split(\" \")[1]\n",
    "        df_name = \"{}_{}\".format(name, food_category)\n",
    "        df.to_csv(\"./{}_비정상종료.csv\".format(df_name), encoding='utf-8-sig')\n",
    "        print(\"종료위치 {} - {} - {}\".format(food_category, urls.index(url), idx))\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    df_name = \"{}_{}\".format(name, food_category)\n",
    "    df.to_csv(\"./{}.csv\".format(df_name), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수집을 위한 main 메소드\n",
    "지역과 카테고리를 지정하며, 소요시간을 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    categories = [\"1인분주문\",\"프랜차이즈\", \"치킨\", \"피자양식\", \"중국집\", \"한식\", \"일식돈까스\", \"족발보쌈\", \"야식\", \"분식\", \"카페디저트\"]\n",
    "    final_start = datetime.datetime.now()\n",
    "\n",
    "    for category in categories:\n",
    "        start = datetime.datetime.now()\n",
    "        yogiyo_crawling('서울특별시 강남구 삼성동 16-1 강남구청', category)\n",
    "        end = datetime.datetime.now()\n",
    "        t = end - start\n",
    "        hours, remainder = divmod(t.seconds, 3600)\n",
    "        print(\"{} : {}시간 {}분\".format(category, hours, remainder))\n",
    "\n",
    "    final_end = datetime.datetime.now()\n",
    "\n",
    "    t = final_end - final_start\n",
    "    hours, remainder = divmod(t.seconds, 3600)\n",
    "    print(final_start)\n",
    "    print(final_end)\n",
    "    print(\"전체소요시간 : {}시간 {}분\".format(hours, remainder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 요기요 가게정보 크롤링\n",
    "\n",
    "각 리뷰별 데이터 뿐만 아니라, 추가 분석을 위한 각 가게별 평점, 메뉴 등 전반적인 가게 정보를 수집한다. 해당 메소드에서 수집하는 데이터는 다음과 같다.\n",
    "\n",
    "- 가게정보\n",
    "    - 가게명\n",
    "    - 총 평점\n",
    "    - 리뷰 개수\n",
    "    - 사장님 리뷰 개수 \n",
    "    - 최소주문금액\n",
    "    - 배달시간\n",
    "    - 배달료\n",
    "    - 맛 평점\n",
    "    - 양 평점\n",
    "    - 배달 평점\n",
    "    - 영업시간\n",
    "    - 위치\n",
    "    - 세스코 유무\n",
    "    \n",
    "    \n",
    "- 메뉴정보\n",
    "    - 가게 : {메뉴명 : 가격} 형태로 가게의 메뉴 정보를 json 형식으로 저장한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가게 정보 크롤링\n",
    "\n",
    "데이터 수집은 전체 가게 리스트에서 수집이 한번에 가능한 정보와 가게별로 페이지에 접속하여 수집한 정보들로 구분된다. 이에 스크롤 조작을 통해 전체 가게 리스트를 수집하고, 기존에 수집한 URL 기반으로 가게별 페이지에 접근하여 이외의 정보를 수집한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yogiyo_crawling_store_info(address, food_category):\n",
    "\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "\n",
    "    # 주소 기준으로 초기화\n",
    "    #driver = webdriver.Chrome('chromedriver')\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "\n",
    "    driver.get(list_url)\n",
    "    time.sleep(4)\n",
    "    element = driver.find_element_by_name(\"address_input\")\n",
    "    element.clear()\n",
    "    element.send_keys(address)\n",
    "    btn = driver.find_element_by_css_selector(\"#button_search_address > button.btn.btn-default.ico-pick\")\n",
    "    btn.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 리뷰 많은 순으로 sorting\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select/option[3]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 초기 데이터 프레임 생성\n",
    "    df = pd.DataFrame(columns=['가게명', '총평점', '리뷰개수', '사장님리뷰개수', '최소주문금액', '배달시간','배달료', \n",
    "                               '맛 평점', '양 평점', '배달 평점', '영업시간', '위치', '세스코유무'])\n",
    "\n",
    "    # 해당 카테고리에 접속하여 마지막까지 스크롤\n",
    "    driver.get(list_url + food_category)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            element.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"끝\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "    \n",
    "    # 모든 가게의 정보를 담고 있는 html 파싱 \n",
    "    main_html = driver.page_source\n",
    "    soup = BeautifulSoup(main_html, 'html.parser')\n",
    "    \n",
    "    #가게 이름 리스트\n",
    "    store_name = soup.find_all(attrs={'class':'restaurant-name ng-binding'})\n",
    "    store_name = list(map(lambda x: x.text, store_name))    \n",
    "    \n",
    "    #총평점 리스트\n",
    "    star_review = soup.find_all(attrs={'ng-show':'restaurant.review_avg > 0'})\n",
    "    star_review = list(map(lambda x: float(x.text[-3:]), star_review))  \n",
    "    \n",
    "    #리뷰개수 리스트 \n",
    "    review_count = soup.find_all(attrs={'ng-show':'restaurant.review_count > 0'})\n",
    "    review_count = list(map(lambda x: int(x.text[30:-23]), review_count))  \n",
    "    \n",
    "    #사장님 댓글 갯수 리스트\n",
    "    owner_count = soup.find_all(attrs={'ng-show':'restaurant.owner_reply_count > 0'})\n",
    "    owner_count = list(map(lambda x: int(x.text[33:-23]),owner_count))  \n",
    "\n",
    "    #최소 주문 금액 리스트\n",
    "    min_price = soup.find_all(attrs={'class':'min-price ng-binding'})\n",
    "    min_price = list(map(lambda x: int(x.text[:-7].replace(',','')), min_price))  \n",
    "\n",
    "    #배달 시간 리스트\n",
    "    delivery_time = soup.find_all(attrs={'ng-show':'restaurant.estimated_delivery_time'})\n",
    "    delivery_time = list(map(lambda x: x.text[25:-23], delivery_time))  \n",
    "    \n",
    "    df['가게명'] = store_name\n",
    "    df['총평점'] = star_review\n",
    "    df['리뷰개수'] = review_count\n",
    "    df['사장님리뷰개수'] = owner_count\n",
    "    df['최소주문금액'] = min_price\n",
    "    df['배달시간'] = delivery_time\n",
    "    \n",
    "    ### 가게정보 파싱 끝 ###\n",
    "\n",
    "    ### 메뉴 및 세부 가게정보 가게별로 파싱###\n",
    "    df.set_index('가게명', inplace = True)\n",
    "\n",
    "    dic3 = {} \n",
    "    df2 = pd.read_csv(\"./\"+address.split(\" \")[1]+\"url/\"+address.split(\" \")[1]+\"_\"+food_category+\"_세부업종.csv\")\n",
    "    urls = df2['url'].tolist()\n",
    "    \n",
    "    # 가게별로 순회\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            driver.get(urls[i])\n",
    "                      \n",
    "            # 메뉴 크롤링\n",
    "            time.sleep(2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            name = soup.find('span', attrs={'class' : \"restaurant-name ng-binding\"}).text\n",
    "            sub_lists = soup.find_all('div', attrs={'class':'panel panel-default ng-scope', 'ng-repeat':\"category in restaurant.menu\"})\n",
    "            sub_lists = sub_lists[1:]\n",
    "\n",
    "            dic2 = {}\n",
    "            for l in sub_lists:\n",
    "                menu_category = l.find('span', attrs={'ng-class':'get_menu_class(category.slug)'}).text\n",
    "                menus = l.find_all('td', attrs={'class' : 'menu-text'})\n",
    "                for menu in menus:\n",
    "                    menu_name = menu.find('div', attrs={'class' : 'menu-name ng-binding'}).text\n",
    "                    menu_price = menu.find('span', attrs={'ng-bind' : 'item.price|krw'}).text\n",
    "                    dic2[menu_name] = menu_price\n",
    "            dic3[name] = dic2\n",
    "\n",
    "            # 평점 크롤링\n",
    "            review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a')\n",
    "            driver.execute_script(\"arguments[0].click();\", review)\n",
    "            time.sleep(1)\n",
    "            taste_star = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[1]/div/ul/li[1]/span[2]/span[6]').text\n",
    "            df.loc[name,'맛 평점'] = taste_star\n",
    "            quantity_star = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[1]/div/ul/li[2]/span[2]/span[6]').text\n",
    "            df.loc[name,'양 평점'] = quantity_star\n",
    "            delivery_star = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[1]/div/ul/li[3]/span[2]/span[6]').text\n",
    "            df.loc[name,'배달 평점'] = delivery_star\n",
    "\n",
    "            # 가게 세부 정보 크롤링\n",
    "            info = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[3]/a')\n",
    "            driver.execute_script(\"arguments[0].click();\", info)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            infos = soup.find_all(attrs={'class':'info-item'})\n",
    "            for info in infos:\n",
    "                if \"업체정보\" in info.text:\n",
    "                    try:\n",
    "                        delivery_fee = soup.find('span', attrs={'class': \"list-group-item clearfix text-right ng-binding\"}).text \n",
    "                    except:\n",
    "                        delivery_fee = 0\n",
    "                      \n",
    "                    print(delivery_fee)\n",
    "                    df.loc[name,'배달료'] = delivery_fee\n",
    "                      \n",
    "                    is_cesco = 0 if info.find('p', attrs={'ng-show':'restaurant.tags.length > 0 && restaurant.tags.indexOf(\"CESCO\") >= 0',\n",
    "                                                          'class':'ng-hide'}) is not None else 1\n",
    "                    print(is_cesco)\n",
    "                    df.loc[name,'세스코유무'] = is_cesco\n",
    "                    opening_hour = info.find('p').find('span').text if \"영업시간\" in info.find('p').text else \"\"\n",
    "                    print(opening_hour)\n",
    "                    df.loc[name,'영업시간'] = opening_hour\n",
    "                    location = info.find('p', attrs={'ng-show':\"restaurant.address.length > 0\"}).find('span').text\n",
    "                    print(location)\n",
    "                    df.loc[name,'위치'] = location\n",
    "                    break\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "        except:\n",
    "            continue # url은 보유하고있으나 이후 가맹 해지된 업장 예외처리\n",
    "    \n",
    "    # json, csv 저장\n",
    "    json_file = './'+ address.split(\" \")[1]+'_'+food_category+'_메뉴정보.json'\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dic3, f, ensure_ascii = False)\n",
    "    \n",
    "    df.to_csv('./'+ address.split(\" \")[1]+'_'+food_category+'_가게정보.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메소드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yogiyo_crawling_store_info('서울특별시 강남구 삼성동 16-1 강남구청', \"족발보쌈\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Find-A",
   "language": "python",
   "name": "finda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
