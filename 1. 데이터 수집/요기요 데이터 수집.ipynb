{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ğŸ” ìš”ê¸°ìš” ë°ì´í„° ìˆ˜ì§‘\n",
    "\n",
    "## ì‚¬ì „ ìì²´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ ìš”ê¸°ìš” ì›¹í˜ì´ì§€ ëŒ€ìƒìœ¼ë¡œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•œë‹¤. \n",
    "### ìˆ˜ì§‘í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "- ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ì‚¬ì „ë‹¨ê³„ë¡œì„œ ê°€ê²Œë³„ URL ìˆ˜ì§‘\n",
    "- ê°€ê²Œë³„ URL ê¸°ë°˜ ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘\n",
    "- ì¶”ê°€ ë¶„ì„ì„ ìœ„í•œ ê°€ê²Œ í‰ì , ë©”ë‰´ ë“± ì „ë°˜ì ì¸ ê°€ê²Œ ì •ë³´ ìˆ˜ì§‘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ìš”ê¸°ìš” ê°€ê²Œë³„ URL ìˆ˜ì§‘\n",
    "\n",
    "## ê°œìš”\n",
    "ìš”ê¸°ìš” í™ˆí˜ì´ì§€ëŠ” ëŒ€ë¶€ë¶„ì˜ ìš”ì†Œê°€ ë™ì  í˜ì´ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì´ì— ë”°ë¼ ë°œìƒí•˜ëŠ” ì´ìŠˆë¥¼ í•´ê²°í•˜ë©°, ë°ì´í„° ìˆ˜ì§‘ì˜ íš¨ìœ¨ì„±ì„ ìœ„í•´ ê°€ê²Œë³„ ì ‘ê·¼ì´ ì§ì ‘ì ìœ¼ë¡œ ê°€ëŠ¥í•˜ë„ë¡ ë¦¬ë·° ë°ì´í„°ì— ìˆ˜ì§‘ì— ì•ì„œ ê°€ê²Œë³„ë¡œ URLì„ ìˆ˜ì§‘í•œë‹¤. \n",
    "\n",
    "\n",
    "- ë¨¼ì € URLì„ ìˆ˜ì§‘í•œ ì²«ë²ˆì§¸ ì´ìœ ëŠ” í•˜ë‚˜ì˜ ê°€ê²Œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•œ í›„ ê¸°ì¡´ì˜ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒì•„ê°€ê²Œ ë  ê²½ìš° ë™ì ìœ¼ë¡œ ìš”ì†Œë“¤ì´ ë¡œë”©ë˜ì–´ ê¸°ì¡´ì˜ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ ìˆœì„œê°€ ë³´ì¥ë˜ì§€ ì•ŠëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•¨ì´ë‹¤. ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ì˜ ìˆœì„œê°€ ë³´ì¥ë˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” ì‹œê°„ëŒ€ì— ë”°ë¼ í˜„ì¬ ì˜¤í”ˆ ì¤‘ì¸ ê°€ê²Œê°€ ë™ì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒìœ„ì— ìœ„ì¹˜í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë©°, ìˆ˜ì§‘í•˜ëŠ” ì–‘ê³¼ ì†Œìš”ë˜ëŠ” ì‹œê°„ì„ ê³ ë ¤í•˜ì˜€ì„ ë•Œ ì¤‘ë³µ ë˜ëŠ” ëˆ„ë½ë˜ëŠ” ê°€ê²Œê°€ ìƒê¸¸ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. \n",
    " \n",
    " \n",
    "\n",
    "- ë‘ë²ˆì§¸ ì´ìœ ëŠ” ìš”ê¸°ìš”ì˜ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ê°€ í˜ì´ì§€ ë§ˆì§€ë§‰ ìŠ¤í¬ë¡¤ë§ˆë‹¤ 60ê°œì”© ìƒˆë¡œ ìƒì„±ë¨ì— ë”°ë¼ ë°œìƒí•˜ëŠ” ë¹„íš¨ìœ¨ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•¨ì´ë‹¤. ìš”ê¸°ìš” í˜ì´ì§€ëŠ” í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ë‹¹ ë§ê²ŒëŠ” ìˆ˜ë°±, ìˆ˜ì²œê°œì˜ ê°€ê²Œê°€ ì¡´ì¬í•œë‹¤. ì´ì— í•˜ë‹¨ì— ìœ„ì¹˜í•˜ëŠ” ê°€ê²Œì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œëŠ” ìƒë‹¹íˆ ë§ì€ íšŸìˆ˜ì˜ ìŠ¤í¬ë¡¤ì´ í•„ìš”í•˜ë©°, ì´ ê³¼ì •ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì§€ì—°, ì¸í„°ë„· ì‚¬ìš©ê¸°ë¡ì˜ ê³¼ë‹¤í•œ ì ì¬ ë“±ì˜ ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ì„ ë•ŒëŠ” ì˜ˆì™¸ì²˜ë¦¬ë¡œë„ ë³µêµ¬í•˜ê¸° ì–´ë ¤ìš´ ì‘ì—… ì†ì‹¤ê³¼ ì‹œê°„ ì†Œìš”ê°€ ë°œìƒí•œë‹¤. íŠ¹íˆ ì²«ë²ˆì§¸ ì´ìœ ì—ì„œ ì–¸ê¸‰í–ˆë“¯ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ê°€ ë™ì ìœ¼ë¡œ ìš”ì†Œë“¤ì´ ì¬ë¡œë”©ë¨ì— ë”°ë¼ í•˜ë‚˜ì˜ ê°€ê²Œ ìˆ˜ì§‘ í›„ ë‹¤ì‹œ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒì•„ì˜¤ëŠ” ê²½ìš° ë‹¤ì‹œ ì²«ë²ˆì§¸ ê°€ê²Œë¶€í„° ë‹¤ìŒ ìˆ˜ì§‘ ëŒ€ìƒ ê°€ê²Œê¹Œì§€ ìŠ¤í¬ë¡¤ì„ í•´ì•¼í•˜ëŠ” ìƒí™©ì´ ë²Œì–´ì§„ë‹¤. ì´ëŠ” í›„ë°˜ë¶€ì˜ ê°€ê²Œë¡œ ê°ˆìˆ˜ë¡ ì‹œê°„ì´ ê¸‰ê²©í•˜ê²Œ ëŠ˜ì–´ë‚˜ í° ë¹„íš¨ìœ¨ì„ ì•¼ê¸°í•˜ê²Œ ëœë‹¤.\n",
    " \n",
    " \n",
    "- ì´ì— ì´ˆê¸°ì— URLì„ ë¯¸ë¦¬ ìˆ˜ì§‘í•´ë‘” í›„ ê°€ê²Œë³„ë¡œ ì ‘ê·¼í•  ë•Œ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ í° ì‹œê°„ ì ˆê° íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í…Œê³ ë¦¬ë³„ ê°€ê²Œ ê°œìˆ˜ ìˆ˜ì§‘\n",
    "\n",
    "ìˆ˜ì§‘ì„ ì›í•˜ëŠ” ì§€ì—­ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ì…ë ¥ ë°›ì•„ í•´ë‹¹ ì§€ì—­ìœ¼ë¡œ ì§€ì—­ì„¤ì •ì„ ë³€ê²½í•œë‹¤. ì´í›„ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì ‘ì†í•˜ì—¬ ë¦¬ë·° ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë ¤ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‚´ì— ê°€ê²Œê°€ ëª‡ê°œ ì¡´ì¬í•˜ëŠ”ì§€ ë°˜í™˜í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_reviews(location, food_category):\n",
    "    location\n",
    "    name = location.split(\" \")[1]\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "    urls = []\n",
    "    df = pd.DataFrame(columns=['url', 'ê°œìˆ˜ì¶©ì¡±'])\n",
    "\n",
    "    #### ì£¼ì†Œ ê¸°ì¤€ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "\n",
    "    driver.get(list_url)\n",
    "    time.sleep(8)\n",
    "    element = driver.find_element_by_name(\"address_input\")\n",
    "    element.clear()\n",
    "    element.send_keys(location)\n",
    "    btn = driver.find_element_by_css_selector(\"#button_search_address > button.btn.btn-default.ico-pick\")\n",
    "    btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # ë¦¬ë·° ë§ì€ ìˆœìœ¼ë¡œ sorting\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select/option[3]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ ì ‘ì† ë° ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ \n",
    "    driver.get(list_url + food_category)\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    print(last_height)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            element.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        time.sleep(1)\n",
    "        if new_height == last_height:\n",
    "            print(\"ë\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "    \n",
    "    store_list = driver.find_elements_by_xpath(\"//div[@class='item clearfix']\")\n",
    "    \n",
    "    return len(store_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ê²Œë³„ URL ìˆ˜ì§‘\n",
    "\n",
    "ìˆ˜ì§‘ì„ ì›í•˜ëŠ” ì§€ì—­, ì¹´í…Œê³ ë¦¬, ì‹œì‘ë²ˆí˜¸, ëë²ˆí˜¸ë¥¼ ì¸ìë¡œ ë°›ì•„ URL ì •ë³´ë¥¼ ìˆ˜ì§‘í•œë‹¤. ì´ ë•Œ href íƒœê·¸ë¥¼ í†µí•œ URL ìˆ˜ì§‘ì´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ í•˜ë‚˜ì˜ ê°€ê²Œì— ì§ì ‘ ì ‘ì† í›„ ë‹¤ì‹œ ë˜ëŒì•„ê°€ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ê²Œ ë˜ê³ , ì•ì„œ ê°œìš”ì—ì„œ ì–¸ê¸‰í•œ ë‘ë²ˆì§¸ ë‹¨ì ì„ ìˆ˜ë°˜í•˜ëŠ” ê²ƒì´ ë¶ˆê°€í”¼í•˜ë‹¤. ì´ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ìˆ˜ë§ì€ ìŠ¤í¬ë¡¤ ì¡°ì‘ì— ì˜í•œ ë¶€í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•˜ì—¬ 100ê°œì”© URLì„ ìˆ˜ì§‘í•˜ë©°, ì´ë¥¼ ìœ„í•˜ì—¬ ì•ì„œ ìˆ˜ì§‘í•œ ì¹´í…Œê³ ë¦¬ë³„ ê°€ê²Œ ê°œìˆ˜ ë°ì´í„°ë¥¼ í™œìš©í•œë‹¤. \n",
    "\n",
    "ì´ ë•Œ ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°ëŠ” \n",
    "- ê°€ê²Œë³„ URL\n",
    "- ê°€ê²Œëª…\n",
    "- ë¦¬ë·°ê°œìˆ˜ ì¶©ì¡± ì—¬ë¶€ì´ë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url3(location, food_category, start, end):\n",
    "    name = location.split(\" \")[1]\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "    urls = []\n",
    "    parsed_data = pd.read_csv(\"./ê°•ë‚¨êµ¬url/ê°•ë‚¨êµ¬_\"+food_category+\"_url.csv\")\n",
    "    stores = parsed_data['ê°€ê²Œëª…'].tolist()\n",
    "    df = pd.DataFrame(columns=['url', 'ê°œìˆ˜ì¶©ì¡±','ê°€ê²Œëª…'])\n",
    "    \n",
    "    #### ì£¼ì†Œ ê¸°ì¤€ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "\n",
    "    driver.get(list_url)\n",
    "    time.sleep(8)\n",
    "    element = driver.find_element_by_name(\"address_input\")\n",
    "    element.clear()\n",
    "    element.send_keys(location)\n",
    "    btn = driver.find_element_by_css_selector(\"#button_search_address > button.btn.btn-default.ico-pick\")\n",
    "    btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # ë¦¬ë·° ë§ì€ ìˆœìœ¼ë¡œ sorting\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select/option[3]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ ì ‘ì† ë° ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ \n",
    "    driver.get(list_url + food_category) \n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        for i in tqdm(range(start, end)):\n",
    "            time.sleep(0.5)\n",
    "            is_valid = 1\n",
    "            path = '//*[@id=\"content\"]/div/div[5]/div/div/div[' + str(i) + ']/div'\n",
    "            \n",
    "            # ê°€ê²Œëª… ìš”ì†Œ ì°¾ê¸°(ìŠ¤í¬ë¡¤ì´ í•„ìš” ì—†ëŠ” ì´ˆê¸° 60ê°œ ë¦¬ìŠ¤íŠ¸)\n",
    "            try:\n",
    "                store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[5]/div/div/div['+str(i)+']/div/table/tbody/tr/td[2]/div/div[1]').text\n",
    "            \n",
    "            # ì§€ì • íšŸìˆ˜ë§Œí¼ ìŠ¤í¬ë¡¤    \n",
    "            except:\n",
    "                for j in range(int(i // 60)):\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    if i<480:\n",
    "                        time.sleep(i//60)\n",
    "                    else:\n",
    "                        time.sleep(8)\n",
    "            \n",
    "            # ê°€ê²Œëª…ê³¼ ë¦¬ë·° ê°œìˆ˜ ì°¾ê¸°           \n",
    "            try:\n",
    "                store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[5]/div/div/div['+str(i)+']/div/table/tbody/tr/td[2]/div/div[1]').text\n",
    "                n = driver.find_element_by_css_selector(\n",
    "                    \"#content > div > div:nth-child(5) > div > div > div:nth-child(\" + str(\n",
    "                        i) + \") > div > table > tbody > tr > td:nth-child(2) > div > div.stars > span:nth-child(2)\").text\n",
    "            \n",
    "            # ì°¾ì§€ ëª»í•œë‹¤ë©´ ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ë¡œ ì•„ì§ ì •ë³´ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ëŒ€ê¸° ë° ì¬ìŠ¤í¬ë¡¤ í›„ ìš”ì†Œ ì°¾ìŒ \n",
    "            except:\n",
    "                time.sleep(6)\n",
    "                store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[5]/div/div/div['+str(i)+']/div/table/tbody/tr/td[2]/div/div[1]').text\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                n = driver.find_element_by_css_selector(\n",
    "                    \"#content > div > div:nth-child(5) > div > div > div:nth-child(\" + str(\n",
    "                        i) + \") > div > table > tbody > tr > td:nth-child(2) > div > div.stars > span:nth-child(2)\").text\n",
    "            \n",
    "            # ì´ë¯¸ ì¤‘ë³µëœ ê°€ê²Œë¼ë©´ ë„˜ì–´ê°€ê¸°\n",
    "            print(store_name)\n",
    "            if store_name in stores:\n",
    "                print(\"skip\")\n",
    "                continue\n",
    "             \n",
    "            # ë¦¬ë·° ê°œìˆ˜ê°€ 10ê°œ ì´ìƒì¸ì§€ í™•ì¸\n",
    "            some_tag = driver.find_element_by_xpath(path)\n",
    "            if n == \"\" or int(n.split(\" \")[1]) < 10:\n",
    "                is_valid = 0\n",
    "\n",
    "            # somthing element ê¹Œì§€ ìŠ¤í¬ë¡¤\n",
    "            # í•´ë‹¹ í˜ì´ì§€ ì ‘ì†\n",
    "            action = ActionChains(driver)\n",
    "            action.move_to_element(some_tag).perform()\n",
    "            driver.find_element_by_xpath(path).click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            row = pd.DataFrame([(driver.current_url, is_valid, store_name)],columns=['url', 'ê°œìˆ˜ì¶©ì¡±','ê°€ê²Œëª…'])\n",
    "            print(row)\n",
    "            df = df.append(row)\n",
    "\n",
    "            driver.back()\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # ì˜ˆì™¸ì²˜ë¦¬ë¡œ í•´ê²°í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œ ë°œìƒ ì‹œ í˜„ì¬ê¹Œì§€ì˜ ì •ë³´ ì €ì¥ \n",
    "    except:\n",
    "        df.to_csv(\"./temp/\"+location.split(\" \")[1]+\"_\"+food_category+\"_url_list2_ë¹„ì •ìƒì¢…ë£Œ-\"+str(start)+\".csv\",\n",
    "              encoding='utf-8-sig')\n",
    "        \n",
    "    # ê²°ê³¼ ì €ì¥    \n",
    "    df.to_csv(\"./temp/\"+location.split(\" \")[1]+\"_\"+food_category+\"_url_list2-\"+str(start)+\".csv\",\n",
    "              encoding='utf-8-sig')\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ê²Œ ì´ë¦„ ìˆ˜ì§‘\n",
    "\n",
    "ì´ˆê¸° URL ìˆ˜ì§‘ ë©”ì†Œë“œëŠ” ì´ë¦„ì„ ì €ì¥í•˜ì§€ ì•Šì•„ ìˆ˜ì§‘ëœ URLì— ì ‘ì†í•´ ê°€ê²Œëª…ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ ê±°ì³¤ë‹¤. ìœ„ì˜ get_url3ë¡œ ë³€ê²½ëœ í›„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(category):\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "    df = pd.read_csv(\"./ê°•ë‚¨êµ¬_\"+category+\"_url_list.csv\")\n",
    "    urls = df['url'].tolist()\n",
    "    store_name = []\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            store_name.append(driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[1]/span').text)\n",
    "        except:\n",
    "            print(url)\n",
    "    df['ê°€ê²Œëª…']=store_name\n",
    "    df.to_csv(\"./ê°•ë‚¨êµ¬_\"+category+\"_url_list.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¹´í…Œê³ ë¦¬ë³„ ê°€ê²Œ ê°œìˆ˜ ìˆ˜ì§‘ \n",
    "dic = {}\n",
    "categories = [\"í”„ëœì°¨ì´ì¦ˆ\", \"í•œì‹\", \"ì¹˜í‚¨\", \"í”¼ìì–‘ì‹\", \"ì¤‘ì‹\", \"ì¼ì‹ëˆê¹ŒìŠ¤\", \"ì¡±ë°œë³´ìŒˆ\", \"ì•¼ì‹\", \"ë¶„ì‹\", \"ì¹´í˜ë””ì €íŠ¸\", \"í¸ì˜ì \"]\n",
    "\n",
    "for cat in categories:\n",
    "    dic[cat] = num_of_reviews(\"ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ 16-1 ê°•ë‚¨êµ¬ì²­\", cat)\n",
    "    \n",
    "'''\n",
    "ê²°ê³¼ \n",
    "\n",
    "dic = {'1ì¸ë¶„ì£¼ë¬¸': 530,\n",
    " 'í”„ëœì°¨ì´ì¦ˆ': 818,\n",
    " 'ì¹˜í‚¨': 331,\n",
    " 'í”¼ìì–‘ì‹': 571,\n",
    " 'ì¤‘ì‹': 210,\n",
    " 'í•œì‹': 1614,\n",
    " 'ì¼ì‹ëˆê¹ŒìŠ¤': 607,\n",
    " 'ì¡±ë°œë³´ìŒˆ': 126,\n",
    " 'ì•¼ì‹': 677,\n",
    " 'ë¶„ì‹': 666,\n",
    " 'ì¹´í˜ë””ì €íŠ¸': 727,\n",
    " 'í¸ì˜ì ': 80}\n",
    "\n",
    "'''\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ë¡œ url ìˆ˜ì§‘\n",
    "# ë§¨ ì•„ë˜ê¹Œì§€ ìŠ¤í¬ë¡¤ ì‹œ 60ê°œì”© ë™ì ìœ¼ë¡œ ê°€ê²Œ ì¶”ê°€ í‘œì‹œë˜ì–´ ì»´í“¨í„°ì— ìƒë‹¹í•œ ë¬´ë¦¬ê°€ ê°€ë©°, í•´ë‹¹ ê°€ê²Œì— ì ‘ì†í•˜ì—¬ url ìˆ˜ì§‘ í›„ ë˜ëŒì•„ì˜¤ë©´ ì„¸ì…˜ì´ ì´ˆê¸°í™” ë¨\n",
    "# ì—ëŸ¬ í•¸ë“¤ë§ì„ ìœ„í•˜ì—¬ 100ê°œì”© ë‚˜ëˆ„ì–´ì„œ ìˆ˜ì§‘\n",
    "for cat in categories:\n",
    "    review_num = dic[cat]\n",
    "    temp = review_num\n",
    "    n = 1\n",
    "    for i in range(review_num//100+1):\n",
    "        print(n, temp)\n",
    "        if temp < 100:\n",
    "            get_url3(\"ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ 16-1 ê°•ë‚¨êµ¬ì²­\", cat, n, review_num+1)\n",
    "            break\n",
    "        get_url3(\"ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ 16-1 ê°•ë‚¨êµ¬ì²­\", cat, n, n+100)\n",
    "        n+=100\n",
    "        temp = review_num - n\n",
    "        \n",
    "\n",
    "# ìˆ˜ì§‘ëœ url ë°ì´í„°ì…‹ì— ê°€ê²Œì´ë¦„ ì¶”ê°€\n",
    "for category in categories:\n",
    "    try:\n",
    "        get_name(category)\n",
    "    except:\n",
    "        print(category)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ìš”ê¸°ìš” ë¦¬ë·° í¬ë¡¤ë§\n",
    "## ê°œìš”\n",
    "\n",
    "ì•ì„œ ìˆ˜ì§‘í•œ URL ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ê²Œë³„ ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤. 11ê°œì˜ ì—…ì¢… ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ì„œ ê¸°ì¤€ì— ì¶©ì¡±í•˜ëŠ” ê°€ê²Œ í˜ì´ì§€ë¡œ ì ‘ì† í›„, í•œ ê±´ì˜ ë¦¬ë·°ì— ëŒ€í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.\n",
    "- ì‹œ \n",
    "- êµ¬\n",
    "- ì—…ì¢…ëª…\n",
    "- ê°€ê²Œëª…\n",
    "- ë…„\n",
    "- ì›”\n",
    "- ì „ì²´ í‰ì \n",
    "- ë§› í‰ì \n",
    "- ì–‘ í‰ì \n",
    "- ë°°ë‹¬ í‰ì \n",
    "- ë¦¬ë·° ë‚´ì—­\n",
    "- ì£¼ë¬¸ ë‚´ì—­"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¦¬ë·° ë°ì´í„° í¬ë¡¤ë§\n",
    "ì§€ì—­ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ì¸ìë¡œ ë°›ì•„ í•´ë‹¹ ì§€ì—­/ì—…ì¢…ì— ëŒ€í•´ URL ê¸°ë°˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yogiyo_crawling(location, food_category):\n",
    "    \n",
    "    # url ê°€ì ¸ì˜¤ê¸°\n",
    "    urls = pd.read_csv(\"./\"+location.split(\" \")[1]+\"_\"+food_category+\"_url_list.csv\")\n",
    "    \n",
    "    # ìœ íš¨í•œ ê°€ê²Œ(ë¦¬ë·°ê°œìˆ˜ 10ê°œ ì´ìƒ)ë§Œ ê°€ì ¸ì˜¤ê¸° \n",
    "    is_valid = urls['ê°œìˆ˜ì¶©ì¡±'] == 1\n",
    "    urls = urls[is_valid]\n",
    "    urls = urls['url'].values.tolist()\n",
    "    \n",
    "    address = location\n",
    "    name = location.split(\" \")[1]\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "    \n",
    "    # ê²°ê³¼ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    df = pd.DataFrame(columns=['ì‹œ', 'êµ¬', 'ì—…ì¢…ëª…', 'ê°€ê²Œëª…', 'ë…„', 'ì›”', 'ì „ì²´í‰ì ', 'ë§› í‰ì ', 'ì–‘ í‰ì ', 'ë°°ë‹¬ í‰ì ', 'ë¦¬ë·° ë‚´ìš©', 'ì£¼ë¬¸ ë‚´ì—­'])\n",
    "\n",
    "    try:\n",
    "        # ê°€ê²Œë³„ë¡œ ì ‘ì†í•˜ì—¬ ë¦¬ë·° ìˆ˜ì§‘\n",
    "        for url in tqdm(urls):\n",
    "            # í•´ë‹¹ ê°€ê²Œ urlë¡œ ì ‘ì†\n",
    "            driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # ë¦¬ë·° ê°œìˆ˜\n",
    "            num_of_review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[2]/div/strong[1]')\n",
    "            \n",
    "            # ê°€ê²Œëª…\n",
    "            store_name = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[1]/span').text\n",
    "            \n",
    "            # ë¦¬ë·°ë¥¼ ë³¼ ìˆ˜ ìˆëŠ” ë²„íŠ¼ ì°¾ê³  í´ë¦­  \n",
    "            try:\n",
    "                review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a')\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a')\n",
    "\n",
    "            driver.execute_script(\"arguments[0].click();\", review)\n",
    "            time.sleep(1)\n",
    "\n",
    "            # ì „ì²´ ë¦¬ë·° ê°œìˆ˜ í™•ì¸    \n",
    "            try:\n",
    "                num_of_review = driver.find_element_by_xpath(\n",
    "                    '//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[2]/div/strong[1]')\n",
    "                if num_of_review == '':\n",
    "                    num_of_review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a/span')\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                num_of_review = driver.find_element_by_xpath(\n",
    "                    '//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[2]/div/strong[1]')\n",
    "                if num_of_review == '':\n",
    "                    num_of_review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a/span')\n",
    "\n",
    "                    \n",
    "            # ë™ì ìœ¼ë¡œ ë³€í•˜ëŠ” ë”ë³´ê¸° ë²„íŠ¼ì˜ ì¸ë±ìŠ¤ ê³„ì‚°(ë”ë³´ê¸° í´ë¦­ ì‹œ 10ê°œì”© ì¦ê°€) ë° ì „ì²´ ë¦¬ë·° í™•ì¸ ê°€ëŠ¥í•  ë•Œ ê¹Œì§€ í´ë¦­      \n",
    "            j = 0\n",
    "            idx = 11\n",
    "            max_idx = int(num_of_review.text) // 10\n",
    "\n",
    "            while True:\n",
    "                if j == max_idx:\n",
    "                    html = driver.page_source\n",
    "                    break\n",
    "                try:\n",
    "                    driver.find_element_by_css_selector('#review > li.list-group-item.btn-more > a').click()\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "                    dates = driver.find_element_by_xpath(\n",
    "                        \"// *[ @ id = 'review'] / li[\" + str(idx) + \"] / div[1] / span[2]\").text\n",
    "                    if \"ì „\" in dates or \"ì–´ì œ\" in dates or int(dates.split(\"ë…„\")[0]) > 2019:\n",
    "                        pass\n",
    "                    elif int(dates.split(\"ë…„\")[0]) == 2019 and int(dates.split(\" \")[1][:-1]) >= 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        html = driver.page_source\n",
    "                        break\n",
    "\n",
    "                except Exception as e: # ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì— ë”°ë¥¸ ì˜ˆì™¸ ë°œìƒ ì‹œ ëŒ€ê¸° í›„ ì¬ì‹¤í–‰\n",
    "                    print(e)\n",
    "                    time.sleep(3)\n",
    "                    driver.find_element_by_css_selector('#review > li.list-group-item.btn-more > a').click()\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "                    dates = driver.find_element_by_xpath(\n",
    "                        \"// *[ @ id = 'review'] / li[\" + str(idx) + \"] / div[1] / span[2]\").text\n",
    "                    if \"ì „\" in dates or \"ì–´ì œ\" in dates or int(dates.split(\"ë…„\")[0]) > 2019:\n",
    "                        pass\n",
    "                    elif int(dates.split(\"ë…„\")[0]) == 2019 and int(dates.split(\" \")[1][:-1]) >= 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        html = driver.page_source\n",
    "                        break\n",
    "                j += 1\n",
    "                idx += 10\n",
    "\n",
    "            # ì „ì²´ ë¦¬ë·° í™•ë³´ í›„ html íŒŒì‹±    \n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # ë¦¬ë·°ë§Œ ê°€ì ¸ì˜¤ê¸° \n",
    "            reviews = soup.find_all(class_='list-group-item star-point ng-scope')\n",
    "            \n",
    "            # ë°ì´í„° ì‚½ì… ì „ ì „ì²˜ë¦¬\n",
    "            for review in reviews:\n",
    "                # ë‚ ì§œ ê³„ì‚°\n",
    "                if \"ì „\" in review.find(class_=\"review-time ng-binding\").get_text() or (\n",
    "                        \"ì–´ì œ\" in review.find(class_=\"review-time ng-binding\").get_text()):\n",
    "                    year = 2021\n",
    "                    month = datetime.date.today().month\n",
    "                else:\n",
    "                    year = int(review.find(class_=\"review-time ng-binding\").get_text().split(\"ë…„\")[0])\n",
    "                    month = int(review.find(class_=\"review-time ng-binding\").get_text().split(\" \")[1][0])\n",
    "\n",
    "                # ë¦¬ë·°     \n",
    "                comment = review.find('p', attrs={'ng-show': 'review.comment'})\n",
    "                comment = comment.get_text().replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "                \n",
    "                # ì£¼ë¬¸ ë©”ë‰´ \n",
    "                menu = review.find('div', attrs={'class': \"order-items default ng-binding\"}).get_text()\n",
    "                \n",
    "                # ì „ì²´ í‰ì  \n",
    "                overall = review.find_all(class_=\"full ng-scope\")\n",
    "                \n",
    "                # ì„¸ë¶€ í‰ì  \n",
    "                points = review.find_all(class_=\"points ng-binding\")\n",
    "                try:\n",
    "                    taste = int(points[0].get_text()) # ë§›í‰ì  \n",
    "                except:\n",
    "                    taste = \"\"\n",
    "                try:\n",
    "                    quantity = int(points[1].get_text()) # ì–‘í‰ì  \n",
    "                except:\n",
    "                    quantity = \"\"\n",
    "                try:\n",
    "                    delivery = int(points[2].get_text()) # ë°°ë‹¬ í‰ì \n",
    "                except:\n",
    "                    delivery = \"\"\n",
    "\n",
    "                # í•˜ë‚˜ì˜ ë¦¬ë·° ë°ì´í„° êµ¬ì„±     \n",
    "                row = pd.DataFrame(\n",
    "                    [(address.split(\" \")[0], address.split(\" \")[1], food_category, store_name, year, month,\n",
    "                      len(overall), taste, quantity, delivery, comment, menu)],\n",
    "                    columns=['ì‹œ', 'êµ¬', 'ì—…ì¢…ëª…', 'ê°€ê²Œëª…', 'ë…„', 'ì›”', 'ì „ì²´í‰ì ', 'ë§› í‰ì ', 'ì–‘ í‰ì ', 'ë°°ë‹¬ í‰ì ', 'ë¦¬ë·° ë‚´ìš©', 'ì£¼ë¬¸ ë‚´ì—­'])\n",
    "                \n",
    "                # ìµœì¢… ë°ì´í„°ì…‹ì— ì¶”ê°€\n",
    "                df = df.append(row)\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "    # ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ë¦¬ë·°ì™€ ë„¤íŠ¸ì›Œí¬ ì§€ì—° ë“±ì˜ ë¬¸ì œë¡œ ë™ì  í˜ì´ì§€ í¬ë¡¤ë§ì˜ ì˜¤ë¥˜ ë‹¤ìˆ˜ ì¦ê°€, ì¢…ë£Œëœ ìœ„ì¹˜ì™€ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•¨ìœ¼ë¡œì¨ ëŒ€ì²˜        \n",
    "    except Exception as e:\n",
    "        name = location.split(\" \")[1]\n",
    "        df_name = \"{}_{}\".format(name, food_category)\n",
    "        df.to_csv(\"./{}_ë¹„ì •ìƒì¢…ë£Œ.csv\".format(df_name), encoding='utf-8-sig')\n",
    "        print(\"ì¢…ë£Œìœ„ì¹˜ {} - {} - {}\".format(food_category, urls.index(url), idx))\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    df_name = \"{}_{}\".format(name, food_category)\n",
    "    df.to_csv(\"./{}.csv\".format(df_name), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìˆ˜ì§‘ì„ ìœ„í•œ main ë©”ì†Œë“œ\n",
    "ì§€ì—­ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ì§€ì •í•˜ë©°, ì†Œìš”ì‹œê°„ì„ ì¸¡ì •í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    categories = [\"1ì¸ë¶„ì£¼ë¬¸\",\"í”„ëœì°¨ì´ì¦ˆ\", \"ì¹˜í‚¨\", \"í”¼ìì–‘ì‹\", \"ì¤‘êµ­ì§‘\", \"í•œì‹\", \"ì¼ì‹ëˆê¹ŒìŠ¤\", \"ì¡±ë°œë³´ìŒˆ\", \"ì•¼ì‹\", \"ë¶„ì‹\", \"ì¹´í˜ë””ì €íŠ¸\"]\n",
    "    final_start = datetime.datetime.now()\n",
    "\n",
    "    for category in categories:\n",
    "        start = datetime.datetime.now()\n",
    "        yogiyo_crawling('ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ 16-1 ê°•ë‚¨êµ¬ì²­', category)\n",
    "        end = datetime.datetime.now()\n",
    "        t = end - start\n",
    "        hours, remainder = divmod(t.seconds, 3600)\n",
    "        print(\"{} : {}ì‹œê°„ {}ë¶„\".format(category, hours, remainder))\n",
    "\n",
    "    final_end = datetime.datetime.now()\n",
    "\n",
    "    t = final_end - final_start\n",
    "    hours, remainder = divmod(t.seconds, 3600)\n",
    "    print(final_start)\n",
    "    print(final_end)\n",
    "    print(\"ì „ì²´ì†Œìš”ì‹œê°„ : {}ì‹œê°„ {}ë¶„\".format(hours, remainder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ìš”ê¸°ìš” ê°€ê²Œì •ë³´ í¬ë¡¤ë§\n",
    "\n",
    "ê° ë¦¬ë·°ë³„ ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼, ì¶”ê°€ ë¶„ì„ì„ ìœ„í•œ ê° ê°€ê²Œë³„ í‰ì , ë©”ë‰´ ë“± ì „ë°˜ì ì¸ ê°€ê²Œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•œë‹¤. í•´ë‹¹ ë©”ì†Œë“œì—ì„œ ìˆ˜ì§‘í•˜ëŠ” ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "- ê°€ê²Œì •ë³´\n",
    "    - ê°€ê²Œëª…\n",
    "    - ì´ í‰ì \n",
    "    - ë¦¬ë·° ê°œìˆ˜\n",
    "    - ì‚¬ì¥ë‹˜ ë¦¬ë·° ê°œìˆ˜ \n",
    "    - ìµœì†Œì£¼ë¬¸ê¸ˆì•¡\n",
    "    - ë°°ë‹¬ì‹œê°„\n",
    "    - ë°°ë‹¬ë£Œ\n",
    "    - ë§› í‰ì \n",
    "    - ì–‘ í‰ì \n",
    "    - ë°°ë‹¬ í‰ì \n",
    "    - ì˜ì—…ì‹œê°„\n",
    "    - ìœ„ì¹˜\n",
    "    - ì„¸ìŠ¤ì½” ìœ ë¬´\n",
    "    \n",
    "    \n",
    "- ë©”ë‰´ì •ë³´\n",
    "    - ê°€ê²Œ : {ë©”ë‰´ëª… : ê°€ê²©} í˜•íƒœë¡œ ê°€ê²Œì˜ ë©”ë‰´ ì •ë³´ë¥¼ json í˜•ì‹ìœ¼ë¡œ ì €ì¥í•œë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ê²Œ ì •ë³´ í¬ë¡¤ë§\n",
    "\n",
    "ë°ì´í„° ìˆ˜ì§‘ì€ ì „ì²´ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì§‘ì´ í•œë²ˆì— ê°€ëŠ¥í•œ ì •ë³´ì™€ ê°€ê²Œë³„ë¡œ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ìˆ˜ì§‘í•œ ì •ë³´ë“¤ë¡œ êµ¬ë¶„ëœë‹¤. ì´ì— ìŠ¤í¬ë¡¤ ì¡°ì‘ì„ í†µí•´ ì „ì²´ ê°€ê²Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³ , ê¸°ì¡´ì— ìˆ˜ì§‘í•œ URL ê¸°ë°˜ìœ¼ë¡œ ê°€ê²Œë³„ í˜ì´ì§€ì— ì ‘ê·¼í•˜ì—¬ ì´ì™¸ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yogiyo_crawling_store_info(address, food_category):\n",
    "\n",
    "    list_url = \"https://www.yogiyo.co.kr/mobile/#/\"\n",
    "\n",
    "    # ì£¼ì†Œ ê¸°ì¤€ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "    #driver = webdriver.Chrome('chromedriver')\n",
    "    driver = webdriver.Chrome('/Users/jijoonghong/Downloads/chromedriver')\n",
    "\n",
    "    driver.get(list_url)\n",
    "    time.sleep(4)\n",
    "    element = driver.find_element_by_name(\"address_input\")\n",
    "    element.clear()\n",
    "    element.send_keys(address)\n",
    "    btn = driver.find_element_by_css_selector(\"#button_search_address > button.btn.btn-default.ico-pick\")\n",
    "    btn.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # ë¦¬ë·° ë§ì€ ìˆœìœ¼ë¡œ sorting\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select/option[3]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[1]/div[2]/div/select').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # ì´ˆê¸° ë°ì´í„° í”„ë ˆì„ ìƒì„±\n",
    "    df = pd.DataFrame(columns=['ê°€ê²Œëª…', 'ì´í‰ì ', 'ë¦¬ë·°ê°œìˆ˜', 'ì‚¬ì¥ë‹˜ë¦¬ë·°ê°œìˆ˜', 'ìµœì†Œì£¼ë¬¸ê¸ˆì•¡', 'ë°°ë‹¬ì‹œê°„','ë°°ë‹¬ë£Œ', \n",
    "                               'ë§› í‰ì ', 'ì–‘ í‰ì ', 'ë°°ë‹¬ í‰ì ', 'ì˜ì—…ì‹œê°„', 'ìœ„ì¹˜', 'ì„¸ìŠ¤ì½”ìœ ë¬´'])\n",
    "\n",
    "    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì ‘ì†í•˜ì—¬ ë§ˆì§€ë§‰ê¹Œì§€ ìŠ¤í¬ë¡¤\n",
    "    driver.get(list_url + food_category)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "\n",
    "        scroll_down = 0\n",
    "        while scroll_down < 10:\n",
    "            element.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            scroll_down += 1\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"ë\")\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "    \n",
    "    # ëª¨ë“  ê°€ê²Œì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” html íŒŒì‹± \n",
    "    main_html = driver.page_source\n",
    "    soup = BeautifulSoup(main_html, 'html.parser')\n",
    "    \n",
    "    #ê°€ê²Œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "    store_name = soup.find_all(attrs={'class':'restaurant-name ng-binding'})\n",
    "    store_name = list(map(lambda x: x.text, store_name))    \n",
    "    \n",
    "    #ì´í‰ì  ë¦¬ìŠ¤íŠ¸\n",
    "    star_review = soup.find_all(attrs={'ng-show':'restaurant.review_avg > 0'})\n",
    "    star_review = list(map(lambda x: float(x.text[-3:]), star_review))  \n",
    "    \n",
    "    #ë¦¬ë·°ê°œìˆ˜ ë¦¬ìŠ¤íŠ¸ \n",
    "    review_count = soup.find_all(attrs={'ng-show':'restaurant.review_count > 0'})\n",
    "    review_count = list(map(lambda x: int(x.text[30:-23]), review_count))  \n",
    "    \n",
    "    #ì‚¬ì¥ë‹˜ ëŒ“ê¸€ ê°¯ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
    "    owner_count = soup.find_all(attrs={'ng-show':'restaurant.owner_reply_count > 0'})\n",
    "    owner_count = list(map(lambda x: int(x.text[33:-23]),owner_count))  \n",
    "\n",
    "    #ìµœì†Œ ì£¼ë¬¸ ê¸ˆì•¡ ë¦¬ìŠ¤íŠ¸\n",
    "    min_price = soup.find_all(attrs={'class':'min-price ng-binding'})\n",
    "    min_price = list(map(lambda x: int(x.text[:-7].replace(',','')), min_price))  \n",
    "\n",
    "    #ë°°ë‹¬ ì‹œê°„ ë¦¬ìŠ¤íŠ¸\n",
    "    delivery_time = soup.find_all(attrs={'ng-show':'restaurant.estimated_delivery_time'})\n",
    "    delivery_time = list(map(lambda x: x.text[25:-23], delivery_time))  \n",
    "    \n",
    "    df['ê°€ê²Œëª…'] = store_name\n",
    "    df['ì´í‰ì '] = star_review\n",
    "    df['ë¦¬ë·°ê°œìˆ˜'] = review_count\n",
    "    df['ì‚¬ì¥ë‹˜ë¦¬ë·°ê°œìˆ˜'] = owner_count\n",
    "    df['ìµœì†Œì£¼ë¬¸ê¸ˆì•¡'] = min_price\n",
    "    df['ë°°ë‹¬ì‹œê°„'] = delivery_time\n",
    "    \n",
    "    ### ê°€ê²Œì •ë³´ íŒŒì‹± ë ###\n",
    "\n",
    "    ### ë©”ë‰´ ë° ì„¸ë¶€ ê°€ê²Œì •ë³´ ê°€ê²Œë³„ë¡œ íŒŒì‹±###\n",
    "    df.set_index('ê°€ê²Œëª…', inplace = True)\n",
    "\n",
    "    dic3 = {} \n",
    "    df2 = pd.read_csv(\"./\"+address.split(\" \")[1]+\"url/\"+address.split(\" \")[1]+\"_\"+food_category+\"_ì„¸ë¶€ì—…ì¢….csv\")\n",
    "    urls = df2['url'].tolist()\n",
    "    \n",
    "    # ê°€ê²Œë³„ë¡œ ìˆœíšŒ\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            driver.get(urls[i])\n",
    "                      \n",
    "            # ë©”ë‰´ í¬ë¡¤ë§\n",
    "            time.sleep(2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            name = soup.find('span', attrs={'class' : \"restaurant-name ng-binding\"}).text\n",
    "            sub_lists = soup.find_all('div', attrs={'class':'panel panel-default ng-scope', 'ng-repeat':\"category in restaurant.menu\"})\n",
    "            sub_lists = sub_lists[1:]\n",
    "\n",
    "            dic2 = {}\n",
    "            for l in sub_lists:\n",
    "                menu_category = l.find('span', attrs={'ng-class':'get_menu_class(category.slug)'}).text\n",
    "                menus = l.find_all('td', attrs={'class' : 'menu-text'})\n",
    "                for menu in menus:\n",
    "                    menu_name = menu.find('div', attrs={'class' : 'menu-name ng-binding'}).text\n",
    "                    menu_price = menu.find('span', attrs={'ng-bind' : 'item.price|krw'}).text\n",
    "                    dic2[menu_name] = menu_price\n",
    "            dic3[name] = dic2\n",
    "\n",
    "            # í‰ì  í¬ë¡¤ë§\n",
    "            review = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[2]/a')\n",
    "            driver.execute_script(\"arguments[0].click();\", review)\n",
    "            time.sleep(1)\n",
    "            taste_star = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[1]/div/ul/li[1]/span[2]/span[6]').text\n",
    "            df.loc[name,'ë§› í‰ì '] = taste_star\n",
    "            quantity_star = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[1]/div/ul/li[2]/span[2]/span[6]').text\n",
    "            df.loc[name,'ì–‘ í‰ì '] = quantity_star\n",
    "            delivery_star = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/div[5]/div[1]/div/ul/li[3]/span[2]/span[6]').text\n",
    "            df.loc[name,'ë°°ë‹¬ í‰ì '] = delivery_star\n",
    "\n",
    "            # ê°€ê²Œ ì„¸ë¶€ ì •ë³´ í¬ë¡¤ë§\n",
    "            info = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div[1]/ul/li[3]/a')\n",
    "            driver.execute_script(\"arguments[0].click();\", info)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            infos = soup.find_all(attrs={'class':'info-item'})\n",
    "            for info in infos:\n",
    "                if \"ì—…ì²´ì •ë³´\" in info.text:\n",
    "                    try:\n",
    "                        delivery_fee = soup.find('span', attrs={'class': \"list-group-item clearfix text-right ng-binding\"}).text \n",
    "                    except:\n",
    "                        delivery_fee = 0\n",
    "                      \n",
    "                    print(delivery_fee)\n",
    "                    df.loc[name,'ë°°ë‹¬ë£Œ'] = delivery_fee\n",
    "                      \n",
    "                    is_cesco = 0 if info.find('p', attrs={'ng-show':'restaurant.tags.length > 0 && restaurant.tags.indexOf(\"CESCO\") >= 0',\n",
    "                                                          'class':'ng-hide'}) is not None else 1\n",
    "                    print(is_cesco)\n",
    "                    df.loc[name,'ì„¸ìŠ¤ì½”ìœ ë¬´'] = is_cesco\n",
    "                    opening_hour = info.find('p').find('span').text if \"ì˜ì—…ì‹œê°„\" in info.find('p').text else \"\"\n",
    "                    print(opening_hour)\n",
    "                    df.loc[name,'ì˜ì—…ì‹œê°„'] = opening_hour\n",
    "                    location = info.find('p', attrs={'ng-show':\"restaurant.address.length > 0\"}).find('span').text\n",
    "                    print(location)\n",
    "                    df.loc[name,'ìœ„ì¹˜'] = location\n",
    "                    break\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "        except:\n",
    "            continue # urlì€ ë³´ìœ í•˜ê³ ìˆìœ¼ë‚˜ ì´í›„ ê°€ë§¹ í•´ì§€ëœ ì—…ì¥ ì˜ˆì™¸ì²˜ë¦¬\n",
    "    \n",
    "    # json, csv ì €ì¥\n",
    "    json_file = './'+ address.split(\" \")[1]+'_'+food_category+'_ë©”ë‰´ì •ë³´.json'\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dic3, f, ensure_ascii = False)\n",
    "    \n",
    "    df.to_csv('./'+ address.split(\" \")[1]+'_'+food_category+'_ê°€ê²Œì •ë³´.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©”ì†Œë“œ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yogiyo_crawling_store_info('ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì‚¼ì„±ë™ 16-1 ê°•ë‚¨êµ¬ì²­', \"ì¡±ë°œë³´ìŒˆ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Find-A",
   "language": "python",
   "name": "finda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
